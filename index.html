<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>S. Mahdi H. Miangoleh</title>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <link rel="stylesheet" href="style.css">

    <script src="script.js" defer></script>

</head>

<body>
    <div class="content">
        <img src="assets/mypic2-wide.jpg" alt="Mahdi Miangoleh" class="profile-pic">
        <h1 align="center">S. Mahdi H. Miangoleh</h1>
        <div class="section">
            <p>I am a <em>Computing Science</em> PhD Candidate at Simon Fraser University (SFU). I do research at
                <strong>Computational Photography Lab</strong> under the supervision of Prof. <a
                    href="http://yaksoy.github.io/">Yağız Aksoy</a>.</p>
            <p>I got my Master's degree in Computing Science at SFU in <strong>Summer 2022</strong>. My MSc Thesis is on
                <a href="http://yaksoy.github.io/bmd-msc/"><em>Boosting Monocular Depth Estimation to High
                        Resolution</em></a>.</p>
            <p>Before joining SFU I got my bachelor's degree in Electrical Engineering-Digital Systems at Sharif
                University of Technology.</p>
            <!-- <div class="horizontal-spacer"></div> -->            
        <!-- </div>
        <div class="section"> -->
            <p class="profile-links">
                <a href="mailto:mahdi_miangoleh@sfu.ca" , title="Email"><i class="fas fa-envelope fa-2x"></i></a>
                <a href="https://github.com/miangoleh/" , title="Github"><i class="fab fa-github fa-2x"></i></a>
                <a href="https://twitter.com/mahdi_miangoleh" , title="Twitter"><i class="fab fa-twitter fa-2x"></i></a>
                <a href="https://scholar.google.ca/citations?user=mqJpOqkAAAAJ&hl=en" , title="Google scholar"><i
                        class="fas fa-graduation-cap fa-2x"></i></a>
                <a href="https://linkedin.com/in/miangoleh" , title="LinkedIn"><i class="fab fa-linkedin fa-2x"></i></a>
                <a href="https://unsplash.com/@miangoleh" , title="Unsplash"><i
                        class="fas fa-camera-retro fa-2x"></i></a>
            </p>
        </div>

        <h2>News</h2>
        <div class="section">
            <ul>
                <li>
                    <span class="date">May, 2023</span>
                    <span>I started an Internship at <strong>Bosch AI</strong> under
                        <a href="https://yuliangguo.github.io/">Yuliang Guo</a>!</span>
                </li>
                <li>
                    <span class="date">Aug, 2022</span>
                    <span>I graduated with a Master's degree in Computing Science
                        from Simon Fraser University (<a href="http://yaksoy.github.io/bmd-msc/">Thesis</a>)!
                        I will be continuing my studies toward a PhD degree under supervision of Prof.
                        <a href="http://yaksoy.github.io/">Yağız Aksoy</a>.</span>
                </li>
                <li>
                    <span class="date">Aug, 2021</span>
                    <span>I started an Internship at <strong>Adobe Research</strong>
                        working with <a href="http://zoyathinks.com/">Zoya Bylinskii</a>!</span>
                </li>
            </ul>
        </div>


        <h2>Publications</h2>
        <div class="subsection">
            <p>
                My research focuses on computational photography, 3D computer vision, learning-based approaches for
                vision and graphics, and realistic image editing. I also have experience in embedded systems and desktop
                front-end application development.
            </p>
            <!-- <div class="horizontal-spacer"></div> -->
        </div>
            <div class="subsection">

            <div class="publication-item">
                <div class="image-container">
                    <a href="https://yuliangguo.github.io/depth-any-camera/">
                        <img src="assets/dac.jpg" alt="DAC" class="publication-image" />
                    </a>
                </div>
                <div class="publication-content">
                    <p>
                        <strong class="pubtitle">Depth Any Camera: Zero-Shot Metric Depth Estimation from Any
                            Camera</strong>
                        <br>
                        Yuliang Guo, Sparsh Garg, <strong>S. Mahdi H. Miangoleh</strong>, Xinyu Huang, Liu Ren
                        <br>
                        Proc <strong class="type2">CVPR 2025</strong>
                        <br>
                        <br>
                        <a href="https://yuliangguo.github.io/depth-any-camera/"><i class="fas fa-globe fa-lg"></i></a>
                        <a href="https://arxiv.org/pdf/2501.02464"><i class="fas fa-file-pdf fa-lg"></i></a>
                        <a href="https://github.com/yuliangguo/depth_any_camera"><i class="fab fa-github fa-lg"></i></a>
                    </p>
                    <p>
                        We introduce Depth Any Camera (DAC), a framework enabling superior zero-shot generalization in
                        metric depth estimation for large FoV cameras, including fisheye and 360°. Tired of collecting
                        new data for specific cameras? DAC maximizes the utility of every existing 3D data for training,
                        regardless of the specific camera types used in new applications.
                    </p>
                </div>
            </div>
            <!-- <div class="horizontal-spacer"></div> -->
            </div>
            <div class="subsection">

            <div class="publication-item">
                <div class="image-container">
                    <a href="https://yaksoy.github.io/sidepth/">
                        <img src="https://yaksoy.github.io/images/research/sidepth.jpg" alt="SIDepth"
                            class="publication-image" />
                    </a>
                </div>
                <div class="publication-content">
                    <p>
                        <strong class="pubtitle">Scale-Invariant Monocular Depth Estimation via SSI Depth</strong>
                        <br>
                        <strong>S. Mahdi H. Miangoleh</strong>, Mahesh Reddy, Yağız Aksoy
                        <br>
                        Proc <strong class="type2">SIGGRAPH 2024</strong>
                        <br>
                        <br>
                        <a href="https://yaksoy.github.io/sidepth/"><i class="fas fa-globe fa-lg"></i></a>
                        <a href="https://yaksoy.github.io/papers/SIG24-SI-Depth.pdf"><i
                                class="fas fa-file-pdf fa-lg"></i></a>
                        <a href="https://github.com/compphoto/sidepth"><i class="fab fa-github fa-lg"></i></a>
                        <a href="https://www.youtube.com/watch?v=R_vW6TjYiEM"><i class="fab fa-youtube fa-lg"></i></a>
                    </p>
                    <p>
                        We present a novel approach that leverages SSI depth inputs to enhance SI depth estimation,
                        streamlining the network's role and facilitating in-the-wild generalization for SI depth
                        estimation while only using a synthetic dataset for training.
                    </p>
                </div>
            </div>

            <!-- <div class="horizontal-spacer"></div> -->
            </div>
            <div class="subsection">

            <div class="publication-item">
                <div class="image-container">
                    <a href="https://yaksoy.github.io/intrinsicCompositing/">
                        <img src="https://yaksoy.github.io/images/research/intrinsicCompositing.jpg"
                            alt="IntrinsicCompositing" class="publication-image" />
                    </a>
                </div>
                <div class="publication-content">
                    <p>
                        <strong class="pubtitle">Intrinsic Harmonization for Illumination-Aware Compositing</strong>
                        <br>
                        Chris Careaga, <strong>S. Mahdi H. Miangoleh</strong>, Yağız Aksoy
                        <br>
                        Proc <strong class="type2">SIGGRAPH Asia 2023</strong>
                        <br>
                        <br>
                        <a href="https://yaksoy.github.io/intrinsicCompositing/"><i class="fas fa-globe fa-lg"></i></a>
                        <a href="https://yaksoy.github.io/papers/SigAsia23-IntrinsicCompositing.pdf"><i
                                class="fas fa-file-pdf fa-lg"></i></a>
                        <a href="https://github.com/compphoto/IntrinsicCompositing"><i
                                class="fab fa-github fa-lg"></i></a>
                        <a href="https://www.youtube.com/watch?v=M9hCUTp8bo4"><i class="fab fa-youtube fa-lg"></i></a>
                    </p>
                    <p>
                        We introduce a self-supervised illumination harmonization approach formulated in the intrinsic
                        image domain. First, we estimate a simple global lighting model from mid-level vision
                        representations to generate a rough shading for the foreground region. A network then refines
                        this inferred shading to generate a harmonious re-shading that aligns with the background scene.
                    </p>
                </div>
            </div>
            <!-- <div class="horizontal-spacer"></div> -->
            </div>
            <div class="subsection">

            <div class="publication-item">
                <div class="image-container">
                    <a href="http://yaksoy.github.io/realisticEditing/">
                        <img src="http://yaksoy.github.io/images/research/realisticEditing.jpg" alt="RealisticEditing"
                            class="publication-image" />
                    </a>
                </div>
                <div class="publication-content">
                    <p>
                        <strong class="pubtitle">Realistic Saliency Guided Image Enhancement</strong>
                        <br>
                        <strong>S. Mahdi H. Miangoleh</strong>, Zoya Bylinskii, Eric Kee, Eli Shechtman, Yağız Aksoy
                        <br>
                        Proc <strong class="type2">CVPR 2023</strong>
                        <br>
                        <br>
                        <a href="http://yaksoy.github.io/realisticEditing/"><i class="fas fa-globe fa-lg"></i></a>
                        <a href="http://yaksoy.github.io/papers/CVPR23-RealisticEditing.pdf"><i
                                class="fas fa-file-pdf fa-lg"></i></a>
                        <a href="https://github.com/compphoto/RealisticImageEnhancement"><i
                                class="fab fa-github fa-lg"></i></a>
                        <a href="https://www.youtube.com/watch?v=5dKUDMnnjuo"><i class="fab fa-youtube fa-lg"></i></a>
                    </p>
                    <p>
                        We train and expliot a problem specific realism network to train a saliency-guided image
                        enhancement network which allows maintaining high realism across varying image types while
                        attenuating distractors and amplifying objects of interest. Our proposed approach offers a
                        viable solution for automating image enhancement and photo cleanup operations.
                    </p>
                </div>
            </div>
            <!-- <div class="horizontal-spacer"></div> -->
            </div>
            <div class="subsection">

            <div class="publication-item">
                <div class="image-container">
                    <a href="http://yaksoy.github.io/highresdepth/">
                        <img src="http://yaksoy.github.io/images/research/highresdepth.jpg" alt="highresdepth"
                            class="publication-image" />
                    </a>
                </div>
                <div class="publication-content">
                    <p>
                        <strong class="pubtitle">Boosting Monocular Depth Estimation Models to High-Resolution via
                            Content-Adaptive Multi-Resolution Merging</strong>
                        <br>
                        <strong>S. Mahdi H. Miangoleh*</strong>, Sebastian Dille*, Long Mai, Sylvain Paris, Yağız Aksoy
                        <br>
                        Proc <strong class="type2">CVPR 2021</strong>
                        <br>
                        <br>
                        <a href="http://yaksoy.github.io/highresdepth/"><i class="fas fa-globe fa-lg"></i></a>
                        <a href="http://yaksoy.github.io/papers/CVPR21-HighResDepth.pdf"><i class="fas fa-file-pdf fa-lg"></i></a>
                        <a href="https://github.com/compphoto/BoostingMonocularDepth"><i class="fab fa-github fa-lg"></i></a>
                        <a href="https://www.youtube.com/watch?v=lDeI17pHlqo"><i class="fab fa-youtube fa-lg"></i></a>
                    </p>
                    <p>
                        We demonstrate that there is a trade-off between a consistent scene structure and the
                        high-frequency details, and merge low- and high-resolution estimations to take advantage of this
                        duality using a simple depth merging network and generate multi-megapixel depth maps with a high
                        level of detail using a pre-trained model.
                    </p>
                </div>
            </div>
            <!-- <div class="horizontal-spacer"></div> -->
            </div>
            <div class="subsection">

            <div class="publication-item">
                <div class="image-container">
                    <a href="https://yaksoy.github.io/2DGraphicsComp/">
                        <img src="https://yaksoy.github.io/images/research/2025Poster-LogoCompositing.jpg"
                            alt="logocompositing" class="publication-image" />
                    </a>
                </div>
                <div class="publication-content">
                    <p>
                        <strong class="pubtitle">Physically-Based Compositing of 2D Graphics</strong>
                        <br>
                        Tyrus Tracey, Stefan Diaconu, <strong>S. Mahdi H. Miangoleh</strong>, Sebastian Dille, Yağız Aksoy
                        <br>
                        Proc <strong class="type2">SIGGRAPH POSTER 2025</strong>
                        <br>
                        <br>
                        <a href="https://yaksoy.github.io/2DGraphicsComp/"><i class="fas fa-globe fa-lg"></i></a>
                        <a href="https://yaksoy.github.io/papers/SIG25p-2DGraphicCompositing.pdf"><i class="fas fa-file-pdf fa-lg"></i></a>
                        <a href="https://github.com/tyrus-tracey/CMPT461-GeomLogoTool"><i class="fab fa-github fa-lg"></i></a>
                        <a href="https://www.youtube.com/watch?v=Z8e22OwXgEs"><i class="fab fa-youtube fa-lg"></i></a>
                    </p>
                    <p>
                        We propose an interactive pipeline that enables the seamless integration of a 2D logo into a target image,
                        adapting to the surface geometry and lighting conditions of the scene to ensure realistic appearance.
                    </p>
                </div>
            </div>
            <!-- <div class="horizontal-spacer"></div> -->
            </div>
            <div class="subsection">

            <div class="publication-item">
                <div class="image-container">
                    <a href="http://yaksoy.github.io/interactiveDepth/">
                        <img src="http://yaksoy.github.io/images/research/interactiveDepth.jpg"
                            alt="interactiveeditdepth" class="publication-image" />
                    </a>
                </div>
                <div class="publication-content">
                    <p>
                        <strong class="pubtitle">Interactive Editing of Monocular Depth</strong>
                        <br>
                        Obumneme Stanley Dukor, <strong>S. Mahdi H. Miangoleh</strong>, Mahesh Kumar Krishna Reddy, Long
                        Mai, Yağız Aksoy
                        <br>
                        Proc <strong class="type2">SIGGRAPH POSTER 2022</strong>
                        <br>
                        <br>
                        <a href="http://yaksoy.github.io/interactiveDepth/"><i class="fas fa-globe fa-lg"></i></a>
                        <a href="http://yaksoy.github.io/papers/SIG22a-interactiveDepth.pdf"><i
                                class="fas fa-file-pdf fa-lg"></i></a>
                        <a href="https://depth-app.netlify.app/editor"><i class="fas fa-desktop fa-lg"></i></a>
                        <a href="https://www.youtube.com/watch?v=C1hrsDypFzM"><i class="fab fa-youtube fa-lg"></i></a>
                    </p>
                    <p>
                        In this work, we present a lightweight, web-based interactive depth editing and visualization
                        tool that adapts low-level conventional image editing operations for geometric manipulation to
                        enable artistic control in the 3D photography workflow.
                    </p>
                </div>
            </div>



        </div>

        <h2>Theses</h2>

        <div class="section">
            <div class="publication-item">
                <div class="image-container">
                    <a href="http://yaksoy.github.io/bmd-msc/">
                        <img src="http://yaksoy.github.io/images/research/mahdimsc.jpg" alt="bmdthesis"
                            class="publication-image" />
                    </a>
                </div>
                <div class="publication-content">
                    <p>
                        <strong class="pubtitle">Boosting Monocular Depth Estimation to High Resolution</strong>
                        <br>
                        <strong>Seyed Mahdi Hosseini Miangoleh</strong>
                        <br>
                        <strong class="type2">MSc Thesis 2022</strong>
                        <br>
                        <br>
                        <a href="http://yaksoy.github.io/bmd-msc/"><i class="fas fa-globe fa-lg"></i></a>
                        <a href="https://sfu.ca/~smh31/masterthesis"><i class="fas fa-file-pdf fa-lg"></i></a>
                        <a href="https://www.youtube.com/watch?v=DZ0ft1l50KY"><i class="fab fa-youtube fa-lg"></i></a>
                    </p>
                    <p>
                        We demonstrate that there is a trade-off between a consistent scene structure and the
                        high-frequency details, and merge low- and high-resolution estimations to take advantage of this
                        duality using a simple depth merging network and generate multi-megapixel depth maps with a high
                        level of detail using a pre-trained model.
                    </p>
                </div>
            </div>
        </div>



        <footer>
            <p id="footer-text" style="text-align: center; margin-top: 20px; font-size: 14px; color: #555;"></p>
        </footer>

</body>

</html>